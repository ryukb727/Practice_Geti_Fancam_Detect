| [Korean ğŸ‡°ğŸ‡·](#korean) | [Japanese ğŸ‡¯ğŸ‡µ](#japanese) | [Geti](#Geti) |
| :---: | :---: | :---: |

</div>

---

<div id="korean">

### ğŸ‡°ğŸ‡· Korean Version
# ğŸ”¬ Intel Getië¥¼ í™œìš©í•œ AI Classification & Detection ì‹¤ìŠµ

ë³¸ ë¬¸ì„œëŠ” **Intel Edge AI SW Academy ê³¼ì •** ì¤‘ ì§„í–‰í•œ  
**Intel Geti í”Œë«í¼ ê¸°ë°˜ AI ì‹¤ìŠµ(Classification / Object Detection)** ë‚´ìš©ì„ ì •ë¦¬í•œ ê²ƒì…ë‹ˆë‹¤.

ë³¸ ì‹¤ìŠµì„ í†µí•´ ëª¨ë¸ í•™ìŠµ ì „ ê³¼ì •ê³¼ í•¨ê»˜,  
**ë°ì´í„°ì˜ ì–‘Â·í’ˆì§ˆÂ·ëª¨ë¸ ì•„í‚¤í…ì²˜ ì„ íƒì´ ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥**ì„ ë¶„ì„í–ˆìŠµë‹ˆë‹¤.

---

## ğŸ¯ ì‹¤ìŠµ ê°œìš” (Overview)

- **í”Œë«í¼:** Intel Geti Platform
- **ì‹¤ìŠµ ëª©í‘œ**
  - ë°ì´í„°ì…‹ êµ¬ì„± ë° ë¼ë²¨ë§
  - Classification / Detection ëª¨ë¸ í•™ìŠµ ë° ê²€ì¦
  - ë°ì´í„°Â·ëª¨ë¸ ë³€ê²½ì— ë”°ë¥¸ ì„±ëŠ¥ ë³€í™” ë¶„ì„

---

## ğŸ›  ê¸°ìˆ  ìŠ¤íƒ (Tech Stack)

- **Platform:** Intel Geti
- **Task:** Classification / Object Detection
- **Models**
  - EfficientNet-B0 (Classification)
  - EfficientNet-V2-S (Classification)
  - MobileNetV2-AT55 (Detection)
- **Workflow**
  - Dataset êµ¬ì„± â†’ Labeling â†’ Training â†’ Validation â†’ ì„±ëŠ¥ ë¹„êµ

---

## ğŸ” ì‹¤ìŠµ 1. Classification â€” *Guess Who Am I*

íŠ¹ì • ì¸ë¬¼ ë¶„ë¥˜(Classification)ë¥¼ ëª©í‘œë¡œ ì§„í–‰í•œ ì‹¤ìŠµìœ¼ë¡œ,  
**ë°ì´í„°ì…‹ í¬ê¸° ì¦ê°€**ì™€ **ëª¨ë¸ ì•„í‚¤í…ì²˜ ë³€ê²½**ì´ Accuracyì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ë¹„êµí–ˆìŠµë‹ˆë‹¤.


### âœ” ì‹¤í—˜ êµ¬ì„± (Experiment Setup)

- **ì‚¬ìš© ëª¨ë¸**
  - EfficientNet-B0
  - EfficientNet-V2-S

- **ë³€ê²½ ìš”ì†Œ**
  - ë°ì´í„°ì…‹ ì´ë¯¸ì§€ ìˆ˜ (37 â†’ 67 â†’ 119)
  - ëª¨ë¸ ì•„í‚¤í…ì²˜ ë³€ê²½ (B0 â†” V2-S)

> ë™ì¼í•œ ë°ì´í„° ì¡°ê±´ì—ì„œ ëª¨ë¸ë³„ ì„±ëŠ¥ ì°¨ì´ë¥¼ ë¹„êµí•˜ì—¬  
> ë°ì´í„°ì™€ ëª¨ë¸ ì„ íƒì´ ì •í™•ë„ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ê²€ì¦

<br>
<br>

### âœ” ì‹¤í—˜ ê²°ê³¼ â‘  â€” EfficientNet-B0

| Version | ë°ì´í„° ìˆ˜ | Accuracy |
|--------|----------|----------|
| Version 1 | 37 Images | 56% |
| Version 2 | 67 Images | 89% |
| Version 3 | 119 Images | **75%** |


### âœ” ì‹¤í—˜ ê²°ê³¼ â‘¡ â€” EfficientNet-V2-S

| Version | ë°ì´í„° ìˆ˜ | Accuracy |
|--------|----------|----------|
| Version 1 | 37 Images | 67% |
| Version 2 | 67 Images | 78% |
| Version 3 | 119 Images | **100%** |

<br>
<br>

### âœ” ë¹„êµ ë¶„ì„

- ë°ì´í„° ìˆ˜ ì¦ê°€ì— ë”°ë¼ ë‘ ëª¨ë¸ ëª¨ë‘ Accuracy í–¥ìƒ ê²½í–¥ í™•ì¸
- **EfficientNet-V2-SëŠ” ë°ì´í„° ì¦ê°€ì— ë”°ë¼ ì„±ëŠ¥ì´ ì•ˆì •ì ìœ¼ë¡œ ìƒìŠ¹**
- EfficientNet-B0ëŠ” Version 3ì—ì„œ Accuracy í•˜ë½ â†’  
  **ë°ì´í„° êµ¬ì„± í¸í–¥ ë° ì¼ë°˜í™” í•œê³„ ê°€ëŠ¥ì„±** í™•ì¸
  

### âœ… Classification ê²°ë¡ 

> Classification ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•´ì„œëŠ”  
> **ì¶©ë¶„í•œ ë°ì´í„° í™•ë³´ + ë¬¸ì œ íŠ¹ì„±ì— ë§ëŠ” ëª¨ë¸ ì„ íƒ**ì´ í•µì‹¬ì„ì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤.

---

## ğŸ¥ ì‹¤ìŠµ 2. Object Detection â€” *Fan Cam*

ì˜ìƒ ë‚´ íŠ¹ì • ì¸ë¬¼ì˜ ì–¼êµ´ ì˜ì—­ì„ íƒì§€í•˜ëŠ” Object Detection ì‹¤ìŠµìœ¼ë¡œ,  
**ì…ë ¥ ë°ì´í„° í•´ìƒë„ì™€ í’ˆì§ˆ, Bounding Box ì„¤ê³„ Detection ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥**ì„ ë¶„ì„í–ˆìŠµë‹ˆë‹¤.


### âœ” ì‹¤í—˜ êµ¬ì„± (Experiment Setup)

- **ì‚¬ìš© ëª¨ë¸:** MobileNetV2-AT55  
- **ëª©í‘œ:** ì˜ìƒ ë‚´ íŠ¹ì • ì¸ë¬¼ì˜ ì–¼êµ´ ì˜ì—­ íƒì§€  
- **ë³€ê²½ ìš”ì†Œ**
  - ì…ë ¥ ì˜ìƒ í•´ìƒë„ (360p â†’ 1080p)
  - **ë°ì´í„°ì…‹ êµ¬ì„±**
    - ë‹¨ì¼ ì§ìº  ìº¡ì²˜ ì´ë¯¸ì§€ â†’  
      ì˜ìƒ ìƒ‰Â·ë°°ê²½ ìƒ‰ì´ ë‹¤ë¥¸ ì—¬ëŸ¬ ì§ìº  ìº¡ì²˜ ì´ë¯¸ì§€ ì¶”ê°€
  - **Bounding Box ë²”ìœ„**
    - ì „ì‹  â†’ ì–¼êµ´ ì˜ì—­ìœ¼ë¡œ ì¶•ì†Œ

<br>
<br>

### ğŸš¨ ë¬¸ì œ ìƒí™© (Version 1 ~ Version 3)

#### âœ” ê°œì„  ê³¼ì •

- ì €í•´ìƒë„(360p) ì˜ìƒ ê¸°ë°˜ ë°ì´í„°ì…‹ìœ¼ë¡œ í•™ìŠµ
- ë™ì¼ ì¡°ê±´ì—ì„œ **ê³ í•´ìƒë„ ì˜ìƒ(1080p)** ë°ì´í„°ì…‹ìœ¼ë¡œ ë³€ê²½
- í•´ìƒë„ì™€ ë°ì´í„° ìˆ˜ë§Œ ê°œì„ í•œ ìƒíƒœì—ì„œ ëª¨ë¸ ì¬í•™ìŠµ

#### âœ” ì •ëŸ‰ í‰ê°€ ê²°ê³¼

| ì¡°ê±´ | Accuracy |
|-----|----------------|
| 360p ì˜ìƒ | 16% |
| 1080p ì˜ìƒ | **81%** |

- ìˆ˜ì¹˜ìƒ Accuracy ì•½ **400% ì´ìƒ í–¥ìƒ**

<br>
<br>

### âš ï¸ ì‹¤ì œ ì„±ëŠ¥ ê²€ì¦ ì‹¤íŒ¨

- Version 3 ëª¨ë¸ì„ ì‹¤ì œ ì„±ëŠ¥ ê²€ì‚¬(Test)ì— ì ìš©í•œ ê²°ê³¼  
  ğŸ‘‰ **Detection Score: 0ì **
- í•™ìŠµ ë°ì´í„°ì™€ ìœ ì‚¬í•œ ì˜ìƒì—ì„œëŠ” íƒì§€ ê°€ëŠ¥  
- ê·¸ëŸ¬ë‚˜ **ì¡°ê¸ˆì´ë¼ë„ ë‹¤ë¥¸ ì˜ìƒ ì…ë ¥ ì‹œ ì „í˜€ íƒì§€ë˜ì§€ ì•ŠëŠ” ë¬¸ì œ ë°œìƒ**

â†’ ì •ëŸ‰ ì§€í‘œì™€ ì‹¤ì œ ì¼ë°˜í™” ì„±ëŠ¥ ê°„ì˜ í° ê´´ë¦¬ í™•ì¸

<br>
<br>

### ğŸ§  ì›ì¸ ë¶„ì„ (Root Cause Analysis)

#### 1. ë°ì´í„° í¸í–¥ ë° Overfitting

- ì•„ì´ëŒ ìŒì•…ë°©ì†¡ íŠ¹ìœ ì˜ **í™”ë ¤í•œ ë°°ê²½**
- ì»¨ì…‰Â·ì˜ìƒ ê°œì„±ì´ ê°•í•œ ëŒ€ìƒì˜ íŠ¹ì„±
- ë‹¨ì¼ ì§ìº  ê¸°ë°˜ ë°ì´í„°ì…‹ êµ¬ì„±

â†’ íŠ¹ì • ì˜ìƒ ìŠ¤íƒ€ì¼ì— ê³¼ë„í•˜ê²Œ ìµœì ë˜ì–´  
**í•™ìŠµ ë°ì´í„° ì™¸ í™˜ê²½ì— ì·¨ì•½í•œ Overfitting ë°œìƒ**

#### 2. Bounding Box ì„¤ì • ë¬¸ì œ

- Version 1~3:
  - ì „ì‹  ì˜ì—­ Bounding Box ì‚¬ìš©
  - ì–¼êµ´ íƒì§€ ëª©ì  ëŒ€ë¹„ ë¶ˆí•„ìš”í•œ íŠ¹ì§• ë‹¤ìˆ˜ í¬í•¨
- 1080p ì˜ìƒì´ë¼ë„ ì–¼êµ´ ì˜ì—­ì˜ ì‹¤ì§ˆ í•´ìƒë„ëŠ” ë‚®ì•„  
  **Bounding Box í¬ê¸°ì™€ ìœ íš¨ í•´ìƒë„ê°€ Detection ì„±ëŠ¥ì— ì§ì ‘ì  ì˜í–¥**

<br>
<br>

### ğŸ”§ ê°œì„  ì‹œë„ (Version 4)

- **ë°ì´í„° ë‹¤ì–‘ì„± í™•ë³´**
  - ì˜ìƒÂ·ë°°ê²½ì´ ë‹¤ë¥¸ ì§ìº  ì˜ìƒ ì¶”ê°€
- **Bounding Box ì¬ì„¤ê³„**
  - ì „ì‹  â†’ ì–¼êµ´ ì¤‘ì‹¬ìœ¼ë¡œ ë²”ìœ„ ì¶•ì†Œ
- ë‹¨ìˆœ Score ìƒìŠ¹ì´ ì•„ë‹Œ  
  **ì‹¤ì œ ì„±ëŠ¥ ê²€ì‚¬ í†µê³¼ë¥¼ ëª©í‘œë¡œ ë°©í–¥ ì „í™˜**

<br>
<br>

### âœ… ìµœì¢… ê²°ê³¼ (Version 4)

| í•­ëª© | ê²°ê³¼ |
|----|----|
| Accuracy | **78%** |
| Detection Score | **95ì ** |

- ìˆ˜ì¹˜ìƒ AccuracyëŠ” Version 3 ëŒ€ë¹„ **3%p ê°ì†Œ**
- ê·¸ëŸ¬ë‚˜ ì‹¤ì œ ê²€ì¦ì—ì„œëŠ”
  - ì•ˆì •ì ì¸ íƒì§€ ì„±ëŠ¥ í™•ë³´
  - ì¼ë°˜í™” ì„±ëŠ¥ ëŒ€í­ ê°œì„ 

<br>
<br>

### âœ” ë¹„êµ ë¶„ì„

- Version 1~3:
  - Accuracy ScoreëŠ” ìƒìŠ¹í–ˆìœ¼ë‚˜ ì‹¤ì œ ì„±ëŠ¥ ê²€ì¦ ì‹¤íŒ¨
- Version 4:
  - Accuracy ScoreëŠ” ì†Œí­ í•˜ë½í–ˆì§€ë§Œ **ì¼ê´€ëœ íƒì§€ ì„±ëŠ¥ í™•ë³´**
- Detectionì—ì„œëŠ”
  **Accuracy Scoreë³´ë‹¤ ë°ì´í„° ë‹¤ì–‘ì„±ê³¼ ì¼ë°˜í™” ì„±ëŠ¥ì´ ë” ì¤‘ìš”í•¨ì„ í™•ì¸**


### âœ… Detection ê²°ë¡ 

> Object Detection ì„±ëŠ¥ì€  
> **í•´ìƒë„Â·ë°ì´í„° ë‹¤ì–‘ì„±Â·Bounding Box ì„¤ê³„ê°€ í•¨ê»˜ ì‘ìš©**í•˜ë©°,  
> **ì •ëŸ‰ ì§€í‘œë³´ë‹¤ ì‹¤ì œ ì„±ëŠ¥ ê²€ì¦ì´ ë” ì¤‘ìš”í•¨**ì„ ì‹¤ë¬´ ê´€ì ì—ì„œ ì²´ê°í–ˆìŠµë‹ˆë‹¤.

---

## ğŸ“ ì‹¤ìŠµì„ í†µí•´ ë°°ìš´ ì 

- **Data-Centric AI ê´€ì  ê°•í™”**
  - ëª¨ë¸ ë³€ê²½ ì—†ì´ë„ ë°ì´í„° ì„¤ê³„ ê°œì„ ë§Œìœ¼ë¡œ ì„±ëŠ¥ì´ í¬ê²Œ ë‹¬ë¼ì§ì„ ê²½í—˜
- **Classification vs Detection ì°¨ì´ ëª…í™•í™”**
  - Classification: ë°ì´í„° ìˆ˜ + ëª¨ë¸ ì•„í‚¤í…ì²˜ ì¤‘ìš”
  - Detection: ë°ì´í„° í’ˆì§ˆÂ·ë‹¤ì–‘ì„±Â·Bounding Box ì„¤ê³„ ì¤‘ìš”
- **ì •ëŸ‰ ì§€í‘œì˜ í•œê³„ ì¸ì‹**
  - í•™ìŠµ Score â‰  ì‹¤ì œ ì„±ëŠ¥
- **Edge AI ì‹¤ë¬´ ê´€ì  í™•ë³´**
  - ë°°í¬ í™˜ê²½ì—ì„œì˜ ì…ë ¥ ì¡°ê±´ ë³€í™”ê°€ ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ ì´í•´

---


<div align="center">
<a href="#japanese">â¬‡ï¸ æ—¥æœ¬èªãƒãƒ¼ã‚¸ãƒ§ãƒ³ã¸ç§»å‹• (Go to Japanese Version) â¬‡ï¸</a>
</div>

</div>

---

<div id="japanese">

### ğŸ‡¯ğŸ‡µ Japanese Version

---

<div align="center">
<a href="#korean">â¬†ï¸ í•œêµ­ì–´ë²„ì „ìœ¼ë¡œ ëŒì•„ê°€ê¸° (Go back to Korean Version) â¬†ï¸</a>
</div>

</div>

---

<div align="center">
<a href="#Geti">â¬‡ï¸ Go to Geti README.md â¬‡ï¸</a>
</div>

</div>

---

<div id="Geti">

# Code deployment
## Table of contents
- [Introduction](#introduction)
- [Prerequisites](#prerequisites)
- [Installation](#Installation)
- [Usage](#usage)
- [Troubleshooting](#troubleshooting)
- [Package contents](#package-contents)


## Introduction

This code deployment .zip archive contains:

1. Inference model(s) for your IntelÂ® Getiâ„¢ project.

2. A sample image or video frame, exported from your project.   

3. A very simple code example to get and visualize the result of inference for your 
   project, on the sample image.
   
4. Jupyter notebooks with instructions and code for running inference locally for your project.

The deployment holds one model for each task in your project, so if for example 
you created a deployment for a `Detection -> Classification` project, it will consist of
both a detection, and a classification model. The IntelÂ® Getiâ„¢ SDK is used to run 
inference for all models in the project's task chain.

This README describes the steps required to get the code sample up and running on your 
machine.

## Prerequisites

- [Python](https://www.python.org/downloads/) (>=3.9, <=3.12)

## Installation

1. Install [prerequisites](#prerequisites). You may also need to 
   [install pip](https://pip.pypa.io/en/stable/installation/). For example, on Ubuntu 
   execute the following command to install Python and pip:

   ```
   sudo apt install python3-dev python3-pip
   ```
   If you already have installed pip before, make sure it is up to date by doing:

   ```
   pip install --upgrade pip
   ```

2. Create a clean virtual environment: <a name="virtual-env-creation"></a>

   One of the possible ways for creating a virtual environment is to use `virtualenv`:

   ```
   python -m pip install virtualenv
   python -m virtualenv <directory_for_environment>
   ```

   Before starting to work inside the virtual environment, it should be activated:

   On Linux and macOS:

   ```
   source <directory_for_environment>/bin/activate
   ```

   On Windows:

   ```
   .\<directory_for_environment>\Scripts\activate
   ```

   Please make sure that the environment contains 
   [wheel](https://pypi.org/project/wheel/) by calling the following command:

   ```
   python -m pip install wheel
   ```

   > **NOTE**: On Linux and macOS, you may need to type `python3` instead of `python`.

3. In your terminal, navigate to the `example_code` directory in the code deployment 
   package.

4. Install requirements in the environment:

   ```
   python -m pip install -r requirements.txt
   ```

5. (Optional) Install the requirements for running the `demo_notebook.ipynb` Juypter notebook:

   ```
   python -m pip install -r requirements-notebook.txt
   ```
   
## Usage
### Local inference
Both `demo.py` script and the `demo_notebook.ipynb` notebook contain a code sample for:

1. Loading the code deployment (and the models it contains) into memory.

2. Loading the `sample_image.jpg`, which is a random image taken from the project you 
   deployed. 

3. Running inference on the sample image.

4. Visualizing the inference results.

### Inference with OpenVINO Model Server
Inference with OpenVINO Model Server (OVMS) is deprecated in IntelÂ® Getiâ„¢ SDK.

To use OVMS, create a new model deployment in IntelÂ® Getiâ„¢ and select "OpenVINO Model Server deployment". 
After downloading the deployment package, follow the included README instructions to run OVMS.

### Running the demo script

In your terminal:

1. Make sure the virtual environment created [above](#virtual-env-creation) is activated.

2. Make sure you are in the `example_code` directory in your terminal.

3. Run the demo using:
  
   ```
   python demo.py
   ```

The script will run inference on the `sample_image.jpg`. A window will pop up that 
displays the image, and the results of the inference visualized on top of it.

### Running the demo notebooks

In your terminal:

1. Make sure the virtual environment created [above](#virtual-env-creation) is activated.

2. Make sure you are in the `example_code` directory in your terminal.

3. Start JupyterLab using:
   
   ```
   jupyter lab
   ```
   
4. This should launch your web browser and take you to the main page of JupyterLab.

Inside JuypterLab:

5. In the sidebar of the JupyterLab interface, double-click on `demo_notebook.ipynb` open one of the notebooks.
   
6. Execute the notebook cell by cell to view the inference results. 


> **NOTE** The `demo_notebook.ipynb` is a great way to explore the `AnnotationScene` 
> object that is returned by the inference. The demo code only has very basic 
> visualization functionality, which may not be sufficient for all use case. For 
> example if your project contains many labels, it may not be able to visualize the 
> results very well. In that case, you should build your own visualization logic 
> based on the `AnnotationScene` returned by the `deployment.infer()` method.

## Troubleshooting

1. If you have access to the Internet through a proxy server only, please use pip 
   with a proxy call as demonstrated by the command below:

   ```
   python -m pip install --proxy http://<usr_name>:<password>@<proxyserver_name>:<port#> <pkg_name>
   ```

2. If you use Anaconda as environment manager, please consider that OpenVINO has 
   limited [Conda support](https://docs.openvino.ai/2021.4/openvino_docs_install_guides_installing_openvino_conda.html). 
   It is still possible to use `conda` to create and activate your python environment, 
   but in that case please use only `pip` (rather than `conda`) as a package manager 
   for installing packages in your environment.

3. If you have problems when you try to use the `pip install` command, please update 
   pip version as per the following command:
   ```
   python -m pip install --upgrade pip
   ```

## Package contents

The code deployment files are structured as follows:

- deployment
    - `project.json`
    - "<title of task 1>"  
        - model
          - `model.xml`
          - `model.bin`
          - `config.json`
        - python
          - model_wrappers
            - `__init__.py`
            - model_wrappers required to run demo
          - `README.md`
          - `LICENSE`
          - `demo.py`
          - `requirements.txt`
    - "<title of task 2>" (Optional)
        - ...
- example_code
    - `demo.py`
    - `demo_notebook.ipynb`
    - `demo_ovms.ipynb`  
    - `README.md`
    - `requirements.txt`
    - `requirements-notebook.txt`
- `sample_image.jpg`
- `LICENSE`

---

<div align="center">
<a href="#korean">â¬†ï¸ í•œêµ­ì–´ë²„ì „ìœ¼ë¡œ ëŒì•„ê°€ê¸° (Go back to Korean Version) â¬†ï¸</a>
</div>

</div>

---
